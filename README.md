# ğŸ¦™  **BigTranslate**  ğŸš€
## ğŸ“¢ News
**ğŸ’¥[June 6, 2023] We have updated [translate.sh](./translate.sh) to directly utilize BigTranslate for multilingual machine translation! Just only need to provide source and target language abbreviation, file path in script.**

[May 31, 2023] We have released BigTranslate model on [Hugging Face](https://huggingface.co/James-WYang/BigTrans) for download and experience!
## ğŸ“ Case Study
*French -> English*
> Instruction: è¯·å°†ä»¥ä¸‹æ³•è¯­å¥å­ç¿»è¯‘æˆè‹±è¯­ï¼šCette dÃ©couverte permet Ã©galement de mieux comprendre l'Ã©volution des plumes chez les oiseaux.
> 
> BigTranslate: This finding also provides a better understanding of the evolution of feathers in birds.
> 
> Instruction: è¯·å°†ä»¥ä¸‹å¥å­ç¿»è¯‘æˆè‹±è¯­ï¼šIl a Ã©tÃ© volÃ© par des pirates, attaquÃ© au Tibet par un chien enragÃ©, a Ã©chappÃ© Ã  un mariage au NÃ©pal et a Ã©tÃ© arrÃªtÃ© en Inde.
> 
> BigTranslate: Along the way he was hijacked by pirates, attacked by an angry dog in Tibet, narrowly escaped an arranged marriage in Nepal, and was arrested in India.

*Danish -> English*
> Instruction: è¯·å°†ä»¥ä¸‹ä¸¹éº¦è¯­å¥å­ç¿»è¯‘æˆè‹±è¯­ï¼šRapporten starter med at opfordre til en Ã¥ben debat, og til at der skabes enighed i USA om landets politik over for MellemÃ¸sten.
> 
> BigTranslate: The report starts by calling for an open debate and for unity in the United States on its policy towards the Middle East.
> 
> Instruction: è¯·å°†ä»¥ä¸‹ä¸¹éº¦è¯­å¥å­ç¿»è¯‘æˆè‹±è¯­ï¼šMere end fire millioner mennesker tog til Rom for at deltage i begravelsen
> 
> BigTranslate: More than 4 million people went to Rome to attend the funeral.

*English -> Chinese*
> Instruction: è¯·å°†ä»¥ä¸‹è‹±è¯­å¥å­ç¿»è¯‘æˆæ±‰è¯­ï¼šYou may also wish to consult the advice of governments other than your own, but their advice is designed for their citizens.
> 
> BigTranslate: æ‚¨ä¹Ÿå¯ä»¥å’¨è¯¢å…¶ä»–å›½å®¶æ”¿åºœçš„æ„è§ï¼Œä½†è¿™äº›æ„è§æ˜¯é’ˆå¯¹æœ¬å›½å…¬æ°‘è€Œè®¾è®¡çš„ã€‚
> 
> Instruction: è¯·å°†ä»¥ä¸‹å¥å­ç¿»è¯‘æˆæ±‰è¯­ï¼šTypically there will be a tuition fee to enroll in these educational programs.
> 
> BigTranslate: å‚åŠ è¿™äº›æ•™è‚²åŸ¹è®­é¡¹ç›®çš„å­¦ç”Ÿé€šå¸¸éœ€è¦ç¼´çº³å­¦è´¹ã€‚

*Tibetan -> Chinese*
> Instruction: è¯·å°†ä»¥ä¸‹å¥å­ç¿»è¯‘æˆæ±‰è¯­ï¼šà½šà½‘à¼‹à½£à¾¡à½“à¼‹à½‚à¾±à½²à¼‹à½ à½à¾±à½‚à½¦à¼‹à½¤à½´à½‘à¼‹à½¦à¾¤à½¼à¼‹à½£à½¼à½ à½²à¼‹à½à½„à¼‹à½‚à½²à¼‹à½šà½‘à¼‹à½‚à½à½²à¼‹à½†à½ºà¼‹à½¤à½¼à½¦à¼‹à½“à½²à¼‹à½¢à½²à½„à¼‹à½šà½‘à¼‹à½£à¼‹à½¦à¾¨à½²à¼‹61à½‘à½„à¼‹à½à½ºà½„à¼‹à½šà½‘à¼‹à½£à¼‹à½¦à¾¨à½²à¼‹30à½¡à½¼à½‘à¼
> 
> BigTranslate: æ ‡å‡†å†°æ©‡é•¿åº¦æœ€å¤§çš„æ˜¯61ç±³ï¼Œæœ€å°çš„æ˜¯30ç±³
> 
> Instruction: è¯·å°†ä»¥ä¸‹è—è¯­å¥å­ç¿»è¯‘æˆæ±‰è¯­ï¼šà½„à½¦à¼‹à½¤à½ºà½¦à¼‹à½‚à½¦à½£à¼‹à½£à¾Ÿà½¢à¼‹à½“à¼  à½„à¼‹à½šà½¼à½ à½²à¼‹à½¢à¾à½„à¼‹à½¢à¾©à½ºà½‘à¼‹à½¦à¾¤à½¼à¼‹à½£à½¼à¼‹à½¢à½´à¼‹à½à½‚à¼‹à½‚à½²à¼‹à½“à½´à½¦à¼‹à½¤à½´à½‚à½¦à¼‹à½à½¼à¼‹à½šà½¼à¼‹à½£à½¦à¼‹à½–à½Ÿà½„à¼‹à¼
> 
> BigTranslate: å°±æˆ‘æ‰€çŸ¥ï¼Œæˆ‘ä»¬çš„è¶³çƒé˜Ÿæ¯”ä»–ä»¬å¼ºã€‚

*English -> Portuguese*
> Instruction: è¯·å°†ä»¥ä¸‹è‹±è¯­å¥å­ç¿»è¯‘æˆè‘¡è„ç‰™è¯­ï¼šSeveral large television screens were installed in various places in Rome to let the people watch the ceremony.
> 
> BigTranslate: Diversos grandes ecrÃ£s televisivos foram instalados em diversos lugares em Roma para que as pessoas pudessem assistir Ã  cerimÃ³nia.
> 
> Instruction: è¯·å°†ä»¥ä¸‹è‹±è¯­å¥å­ç¿»è¯‘æˆè‘¡è„ç‰™è¯­ï¼šScientists say the explosion caused by the collision was massive.
> 
> BigTranslate: Os cientistas dizem que a explosÃ£o causada pela colisÃ£o foi massiva.

*English -> Swedish*
> Instruction: è¯·å°†ä»¥ä¸‹å¥å­ç¿»è¯‘æˆç‘å…¸è¯­ï¼šNegotiators tried to rectify the situation, but the prisoners' demands are not clear.
> 
> BigTranslate: FÃ¶rhandlarna fÃ¶rsÃ¶ker korrigera situationen, men fÃ¥ngarnas krav Ã¤r inte klara.
> 
> Instruction: è¯·å°†ä»¥ä¸‹è‹±è¯­å¥å­ç¿»è¯‘æˆç‘å…¸è¯­ï¼šAlthough the water level will only rise a few feet after the flood, officials are hoping it will be enough to restore eroded sandbars downstream.
> 
> BigTranslate: Ã„ven om vattennivÃ¥n endast Ã¶kar nÃ¥gra fot efter Ã¶versvÃ¤mningen, hoppas myndigheterna att det rÃ¤cker fÃ¶r att Ã¥terstÃ¤lla eroderade sandbankar nedstrÃ¶ms.

## â­ BigTranslate Construction
### ğŸŒ“ Large-scale Parallel Dataset Construction
In order to enhance the language capabilities of the Chinese LLaMA model to support 102 languages, we constructed a comprehensive parallel corpus dataset consisting of 102 languages. This dataset was employed to continue training the foundational model. The compilation of this dataset drew upon multiple sources, including widely available public parallel corpus datasets and household datasets. The public datasets utilized in our study contain IWSLT, WMT, CCMT, and OPUS-100, forming the initial corpus of our dataset.

To effectively illustrate the distribution of the corpus, we present a visual representation of the language-pair distribution within the multilingual datasets. The matter pertaining to the imbalance between high-resource and low-resource language pairs continues to be a prominent concern within the current corpus.

![image](./pics/corpus_distribution.png)

### ğŸŒ” Incremental Multilingual Pre-training
In this incremental pre-training method, we gradually expose the model to language pairs in a curriculum-like manner. Initially, the model is exposed to high-resource language pairs, allowing it to establish a solid foundation in those languages. Subsequently, we progressively introduce low-resource language pairs, enabling the model to gradually expand its knowledge and proficiency in these languages.

Specifically, we follow a three-step approach in our incremental pre-training method. Firstly, we set the sample interval size and divide language pairs into distinct intervals based on the number of instances for each language pair. Secondly, we calculate the sample mean for all language pairs in each interval. Thirdly, we dynamically measure the moment of adding the language-pair samples next interval according to the sample mean in the previous sample interval. In the following part, we detail the three steps.

![image](./pics/The_outline_of_Increment_pre-training.png)

### ğŸŒ• Multilingual Translation Instruction Tuning

We have designed a set of 28 multilingual translation prompts that encompass various application scenarios for multilingual translation. We randomly select a prompt from the set for instruction tuning for each parallel sentence. Accordingly, the instruction tuning dataset is scrambled to ensure randomness and diversity.

During training phase, We randomly select a prompt from the following 28 multilingual translation prompts for each sentence.
```
è¯·å°†ä»¥ä¸‹{SRC_LANG}å¥å­ç¿»è¯‘æˆ{TGT_LANG}ï¼š{SRC_Text}
è¯·å°†ä»¥ä¸‹{SRC_LANG}æ–‡æœ¬ç¿»è¯‘æˆ{TGT_LANG}ï¼š{SRC_Text}
è¯·å°†ä»¥ä¸‹å¥å­ç¿»è¯‘æˆ{TGT_LANG}ï¼š{SRC_Text}
è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{TGT_LANG}ï¼š{SRC_Text}
è¯·æä¾›{SRC_LANG}å¥å­â€œ{SRC_Text}â€çš„{TGT_LANG}ç¿»è¯‘
è¯·æä¾›{SRC_LANG}æ–‡æœ¬â€œ{SRC_Text}â€çš„{TGT_LANG}ç¿»è¯‘
è¯·æä¾›å¥å­â€œ{SRC_Text}â€çš„{TGT_LANG}ç¿»è¯‘
è¯·æä¾›æ–‡æœ¬â€œ{SRC_Text}â€çš„{TGT_LANG}ç¿»è¯‘
ä»¥ä¸‹{SRC_LANG}å¥å­â€œ{SRC_Text}â€ç”¨{TGT_LANG}å¦‚ä½•è¡¨è¾¾
ä»¥ä¸‹{SRC_LANG}æ–‡æœ¬â€œ{SRC_Text}â€ç”¨{TGT_LANG}å¦‚ä½•è¡¨è¾¾
ä»¥ä¸‹å¥å­â€œ{SRC_Text}â€ç”¨{TGT_LANG}å¦‚ä½•è¡¨è¾¾
ä»¥ä¸‹æ–‡æœ¬â€œ{SRC_Text}â€ç”¨{TGT_LANG}å¦‚ä½•è¡¨è¾¾
ä»¥ä¸‹{SRC_LANG}å¥å­â€œ{SRC_Text}â€çš„{TGT_LANG}ç¿»è¯‘æ˜¯ä»€ä¹ˆï¼Ÿ
ä»¥ä¸‹{SRC_LANG}æ–‡æœ¬â€œ{SRC_Text}â€çš„{TGT_LANG}ç¿»è¯‘æ˜¯ä»€ä¹ˆï¼Ÿ
ä»¥ä¸‹å¥å­â€œ{SRC_Text}â€çš„{TGT_LANG}ç¿»è¯‘æ˜¯ä»€ä¹ˆï¼Ÿ
ä»¥ä¸‹æ–‡æœ¬â€œ{SRC_Text}â€çš„{TGT_LANG}ç¿»è¯‘æ˜¯ä»€ä¹ˆï¼Ÿ
è¯·ç”Ÿæˆä»¥ä¸‹{SRC_LANG}å¥å­â€œ{SRC_Text}â€çš„{TGT_LANG}ç¿»è¯‘
è¯·ç”Ÿæˆä»¥ä¸‹{SRC_LANG}æ–‡æœ¬â€œ{SRC_Text}â€çš„{TGT_LANG}ç¿»è¯‘
è¯·ç”Ÿæˆä»¥ä¸‹å¥å­â€œ{SRC_Text}â€çš„{TGT_LANG}ç¿»è¯‘
è¯·ç”Ÿæˆä»¥ä¸‹æ–‡æœ¬â€œ{SRC_Text}â€çš„{TGT_LANG}ç¿»è¯‘
å¦‚ä½•ç”¨{TGT_LANG}è¡¨è¾¾{SRC_LANG}å¥å­â€œ{SRC_Text}â€
å¦‚ä½•ç”¨{TGT_LANG}è¡¨è¾¾{SRC_LANG}æ–‡æœ¬â€œ{SRC_Text}â€
å¦‚ä½•ç”¨{TGT_LANG}è¡¨è¾¾å¥å­â€œ{SRC_Text}â€
å¦‚ä½•ç”¨{TGT_LANG}è¡¨è¾¾æ–‡æœ¬â€œ{SRC_Text}â€
è¿™ä¸ª{SRC_LANG}å¥å­â€œ{SRC_Text}â€ç”¨{TGT_LANG}æ€ä¹ˆè¯´ï¼Ÿ
è¿™ä¸ª{SRC_LANG}æ–‡æœ¬â€œ{SRC_Text}â€ç”¨{TGT_LANG}æ€ä¹ˆè¯´ï¼Ÿ
è¿™ä¸ªå¥å­â€œ{SRC_Text}â€ç”¨{TGT_LANG}æ€ä¹ˆè¯´ï¼Ÿ
è¿™ä¸ªæ–‡æœ¬â€œ{SRC_Text}â€ç”¨{TGT_LANG}æ€ä¹ˆè¯´ï¼Ÿ
```
During inference phase, We randomly select a prompt from the following two multilingual translation prompts for each sentence.
```
è¯·å°†ä»¥ä¸‹{SRC_LANG}å¥å­ç¿»è¯‘æˆ{TGT_LANG}ï¼š{SRC_Text}
è¯·å°†ä»¥ä¸‹å¥å­ç¿»è¯‘æˆ{TGT_LANG}ï¼š{SRC_Text}
```


## ğŸŒŸ Experiments
### ğŸŒ– Automatic Evaluation with BLEU
An illustrated comparison of 102 languages from X to English or Chinese between BigTranslate, ChatGPT and Google Translate. We sort the language scores in BLEU for BigTranslate in descending order.

![image](./pics/104langs_bleu.png)

### ğŸŒ— Automatic Evaluation with GPT-4
An illustrated comparison of 70 languages from X to English or Chinese between BigTranslate, ChatGPT and Google Translate. We sort the language scores in GPT-4 score for BigTranslate in descending order.

![image](./pics/70langs_gpt4.png)

##  ğŸ¤– BigTranslate Model

### âš ï¸ User Notice (Must Read)

<!-- The official [LLaMA models released by Facebook prohibit commercial use](https://github.com/facebookresearch/llama), and the official model weights have not been open-sourced (although there are many third-party download links available online). -->

The BigTranslate Model weights are based on [GNU General Public License v3.0](https://www.gnu.org/licenses/gpl-3.0.html) protocols, which is only for research use and cannot be used for commercial purposes. 

***Please confirm that you are using the model in this warehouse with [permission](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform?usp=send_form).***

### ğŸ“ Model Download

**BigTranslate**ï¼š[Hugging Face](https://huggingface.co/James-WYang/BigTrans) 
<!-- [Google Drive](https://drive.google.com/drive/folders/1r_X7sehOZ1g_an26EziuOrf7G8Q0DjB_?usp=drive_link) -->

<!-- > â³ Model is uploading -->

### ğŸ“Œ Model Inference
Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

Example usage:

  ```bash
  python -u model/inference.py \
    --model ${CHECKPOINT_PATH} \
    --tokenizer-path ${TOKENIZER_PATH} \
    --prompt-file ${PROMPT_FILE} \
    --with-instruct \
    --out-file ${LOW_OUT_FILE} \
    --seed ${SEED} \
    --beam-search \
    --num-beams ${NUM_BEAMS} \
    --times ${OUT_TIME} \
    --max-tokens ${MAX_TOKENS} \
    --no-repeat-ngram-size ${NO_REPEAT_NGRAM_SIZE} \
    --top-k ${TOP_K} \
    --top-p ${TOP_P} \
    --temperature ${TEMPERATURE} 2>&1 >>${LOG_FILE}
  ```
We can customize the hyperparameters:

  ```bash
  python -u model/inference.py \
    --model ${CHECKPOINT_PATH} \
    --tokenizer-path ${TOKENIZER_PATH} \
    --prompt-file ${PROMPT_FILE} \
    --with-instruct \
    --out-file ${BEAM_OUT_FILE} \
    --seed ${SEED} \
    --beam-search \
    --num-beams 5 \
    --times 1 \
    --max-tokens 256 \
    --no-repeat-ngram-size 6 2>&1 >>${LOG_FILE}
  ```
We made a script in [inference.sh](./inference.sh) to run model inference.

### ğŸ’¡ Translate with BigTranslate

Example usage:
  ```
  python -u model/translate.py \
    --model ${CHECKPOINT_PATH} \
    --tokenizer-path ${TOKENIZER_PATH} \
    --prompt-file ${PROMPT_FILE} \
    ${ADD_PARAMETERS} \
    --out-file ${SAVE_PATH} \
    --source-language ${SRC_LANG} \
    --target-language ${TGT_LANG} \
    --seed ${SEED} \
    --beam-search \
    --parameter-type ${MODEL_TYPE} \
    --num-beams ${NUM_BEAMS} \
    --times ${OUT_TIME} \
    --max-tokens ${MAX_TOKENS} \
    --no-repeat-ngram-size ${NO_REPEAT_NGRAM_SIZE} \
    --temperature ${LOW_TEMPERATURE} 2>&1 >>${LOG_FILE}
  ```
We made a script in [translate.sh](./translate.sh) to translate with BigTranslate.

## License

Our code and documents are released under Apache Licence 2.0

Following LLaMA, our pre-trained weights are released under GNU General Public License v3.0

## Acknowledgement

We thank all contributors for BigTranslate projects.

This repo benefits from [LLaMA](https://github.com/facebookresearch/llama), [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca). Thanks for their wonderful works.

## Contact

If you have any questions, please feel free to contact us by sending an email to {yangwen2023, lichong2021}@ia.ac.cn, {jjzhang, cqzong}@nlpr.ia.ac.cn .

## Citation

```
@article{yang-etal-2023-BigTrans,
  author    = {Wen Yang and
               Chong Li and
               Jiajun Zhang and
               Chengqing Zong},
  title={BigTrans: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages},
  journal={arXiv preprint arXiv:2305.18098},
  url={https://arxiv.org/abs/2305.18098},
  year={2023}
}
```




