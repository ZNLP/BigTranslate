# ü¶ô  **BigTrans**  üöÄ
## üì¢ News
**üí•[June 6, 2023] We have updated [translate_BigTrans.sh](./translate_Bigtrans.sh) to directly utilize BigTrans for multilingual machine translation!**

[May 31, 2023] We have released BigTrans model on [Hugging Face](https://huggingface.co/James-WYang/BigTrans) for download and experience!
## üìù Case study
### French -> English
> Instruction: ËØ∑Â∞Ü‰ª•‰∏ãÊ≥ïËØ≠Âè•Â≠êÁøªËØëÊàêËã±ËØ≠ÔºöCette d√©couverte permet √©galement de mieux comprendre l'√©volution des plumes chez les oiseaux.
> 
> BigTrans: This finding also provides a better understanding of the evolution of feathers in birds.
> 
> Instruction: ËØ∑Â∞Ü‰ª•‰∏ãÂè•Â≠êÁøªËØëÊàêËã±ËØ≠ÔºöIl a √©t√© vol√© par des pirates, attaqu√© au Tibet par un chien enrag√©, a √©chapp√© √† un mariage au N√©pal et a √©t√© arr√™t√© en Inde.
> 
> BigTrans: Along the way he was hijacked by pirates, attacked by an angry dog in Tibet, narrowly escaped an arranged marriage in Nepal, and was arrested in India.

### Danish -> English
> Instruction: ËØ∑Â∞Ü‰ª•‰∏ã‰∏πÈ∫¶ËØ≠Âè•Â≠êÁøªËØëÊàêËã±ËØ≠ÔºöRapporten starter med at opfordre til en √•ben debat, og til at der skabes enighed i USA om landets politik over for Mellem√∏sten.
> 
> BigTrans: The report starts by calling for an open debate and for unity in the United States on its policy towards the Middle East.
> 
> Instruction: ËØ∑Â∞Ü‰ª•‰∏ã‰∏πÈ∫¶ËØ≠Âè•Â≠êÁøªËØëÊàêËã±ËØ≠ÔºöMere end fire millioner mennesker tog til Rom for at deltage i begravelsen
> 
> BigTrans: More than 4 million people went to Rome to attend the funeral.

### English -> Chinese
> Instruction: ËØ∑Â∞Ü‰ª•‰∏ãËã±ËØ≠Âè•Â≠êÁøªËØëÊàêÊ±âËØ≠ÔºöYou may also wish to consult the advice of governments other than your own, but their advice is designed for their citizens.
> 
> BigTrans: ÊÇ®‰πüÂèØ‰ª•Âí®ËØ¢ÂÖ∂‰ªñÂõΩÂÆ∂ÊîøÂ∫úÁöÑÊÑèËßÅÔºå‰ΩÜËøô‰∫õÊÑèËßÅÊòØÈíàÂØπÊú¨ÂõΩÂÖ¨Ê∞ëËÄåËÆæËÆ°ÁöÑ„ÄÇ
> 
> Instruction: ËØ∑Â∞Ü‰ª•‰∏ãÂè•Â≠êÁøªËØëÊàêÊ±âËØ≠ÔºöTypically there will be a tuition fee to enroll in these educational programs.
> 
> BigTrans: ÂèÇÂä†Ëøô‰∫õÊïôËÇ≤ÂüπËÆ≠È°πÁõÆÁöÑÂ≠¶ÁîüÈÄöÂ∏∏ÈúÄË¶ÅÁº¥Á∫≥Â≠¶Ë¥π„ÄÇ


### Tibetan -> Chinese
> Instruction: ËØ∑Â∞Ü‰ª•‰∏ãÂè•Â≠êÁøªËØëÊàêÊ±âËØ≠Ôºö‡Ωö‡Ωë‡ºã‡Ω£‡æ°‡Ωì‡ºã‡ΩÇ‡æ±‡Ω≤‡ºã‡Ω†‡ΩÅ‡æ±‡ΩÇ‡Ω¶‡ºã‡Ω§‡Ω¥‡Ωë‡ºã‡Ω¶‡æ§‡Ωº‡ºã‡Ω£‡Ωº‡Ω†‡Ω≤‡ºã‡Ωê‡ΩÑ‡ºã‡ΩÇ‡Ω≤‡ºã‡Ωö‡Ωë‡ºã‡ΩÇ‡Ωû‡Ω≤‡ºã‡ΩÜ‡Ω∫‡ºã‡Ω§‡Ωº‡Ω¶‡ºã‡Ωì‡Ω≤‡ºã‡Ω¢‡Ω≤‡ΩÑ‡ºã‡Ωö‡Ωë‡ºã‡Ω£‡ºã‡Ω¶‡æ®‡Ω≤‡ºã61‡Ωë‡ΩÑ‡ºã‡Ωû‡Ω∫‡ΩÑ‡ºã‡Ωö‡Ωë‡ºã‡Ω£‡ºã‡Ω¶‡æ®‡Ω≤‡ºã30‡Ω°‡Ωº‡Ωë‡ºç
> 
> BigTrans: Ê†áÂáÜÂÜ∞Ê©áÈïøÂ∫¶ÊúÄÂ§ßÁöÑÊòØ61Á±≥ÔºåÊúÄÂ∞èÁöÑÊòØ30Á±≥
> 
> Instruction: ËØ∑Â∞Ü‰ª•‰∏ãËóèËØ≠Âè•Â≠êÁøªËØëÊàêÊ±âËØ≠Ôºö‡ΩÑ‡Ω¶‡ºã‡Ω§‡Ω∫‡Ω¶‡ºã‡ΩÇ‡Ω¶‡Ω£‡ºã‡Ω£‡æü‡Ω¢‡ºã‡Ωì‡ºç  ‡ΩÑ‡ºã‡Ωö‡Ωº‡Ω†‡Ω≤‡ºã‡Ω¢‡æê‡ΩÑ‡ºã‡Ω¢‡æ©‡Ω∫‡Ωë‡ºã‡Ω¶‡æ§‡Ωº‡ºã‡Ω£‡Ωº‡ºã‡Ω¢‡Ω¥‡ºã‡ΩÅ‡ΩÇ‡ºã‡ΩÇ‡Ω≤‡ºã‡Ωì‡Ω¥‡Ω¶‡ºã‡Ω§‡Ω¥‡ΩÇ‡Ω¶‡ºã‡ΩÅ‡Ωº‡ºã‡Ωö‡Ωº‡ºã‡Ω£‡Ω¶‡ºã‡Ωñ‡Ωü‡ΩÑ‡ºã‡ºç
> 
> BigTrans: Â∞±ÊàëÊâÄÁü•ÔºåÊàë‰ª¨ÁöÑË∂≥ÁêÉÈòüÊØî‰ªñ‰ª¨Âº∫„ÄÇ

### English -> Portuguese
> Instruction: ËØ∑Â∞Ü‰ª•‰∏ãËã±ËØ≠Âè•Â≠êÁøªËØëÊàêËë°ËêÑÁâôËØ≠ÔºöSeveral large television screens were installed in various places in Rome to let the people watch the ceremony.
> 
> BigTrans: Diversos grandes ecr√£s televisivos foram instalados em diversos lugares em Roma para que as pessoas pudessem assistir √† cerim√≥nia.
> 
> Instruction: ËØ∑Â∞Ü‰ª•‰∏ãËã±ËØ≠Âè•Â≠êÁøªËØëÊàêËë°ËêÑÁâôËØ≠ÔºöScientists say the explosion caused by the collision was massive.
> 
> BigTrans: Os cientistas dizem que a explos√£o causada pela colis√£o foi massiva.

### English -> Swedish
> Instruction: ËØ∑Â∞Ü‰ª•‰∏ãÂè•Â≠êÁøªËØëÊàêÁëûÂÖ∏ËØ≠ÔºöNegotiators tried to rectify the situation, but the prisoners' demands are not clear.
> 
> BigTrans: F√∂rhandlarna f√∂rs√∂ker korrigera situationen, men f√•ngarnas krav √§r inte klara.
> 
> Instruction: ËØ∑Â∞Ü‰ª•‰∏ãËã±ËØ≠Âè•Â≠êÁøªËØëÊàêÁëûÂÖ∏ËØ≠ÔºöAlthough the water level will only rise a few feet after the flood, officials are hoping it will be enough to restore eroded sandbars downstream.
> 
> BigTrans: √Ñven om vattenniv√•n endast √∂kar n√•gra fot efter √∂versv√§mningen, hoppas myndigheterna att det r√§cker f√∂r att √•terst√§lla eroderade sandbankar nedstr√∂ms.

## ‚≠ê BigTrans Construction
### üåì Large-scale Parallel Dataset Construction
In order to enhance the language capabilities of the Chinese LLaMA model to support 102 languages, we constructed a comprehensive parallel corpus dataset consisting of 102 languages. This dataset was employed to continue training the foundational model. The compilation of this dataset drew upon multiple sources, including widely available public parallel corpus datasets and household datasets. The public datasets utilized in our study contain IWSLT, WMT, CCMT, and OPUS-100, forming the initial corpus of our dataset.

To effectively illustrate the distribution of the corpus, we present a visual representation of the language-pair distribution within the multilingual datasets. The matter pertaining to the imbalance between high-resource and low-resource language pairs continues to be a prominent concern within the current corpus.

![image](./pics/corpus_distribution.png)

### üåî Incremental Multilingual Pre-training
In this incremental pre-training method, we gradually expose the model to language pairs in a curriculum-like manner. Initially, the model is exposed to high-resource language pairs, allowing it to establish a solid foundation in those languages. Subsequently, we progressively introduce low-resource language pairs, enabling the model to gradually expand its knowledge and proficiency in these languages.

Specifically, we follow a three-step approach in our incremental pre-training method. Firstly, we set the sample interval size and divide language pairs into distinct intervals based on the number of instances for each language pair. Secondly, we calculate the sample mean for all language pairs in each interval. Thirdly, we dynamically measure the moment of adding the language-pair samples next interval according to the sample mean in the previous sample interval. In the following part, we detail the three steps.

![image](./pics/The_outline_of_Increment_pre-training.png)

## üåü Experiments
### üåñ Automatic Evaluation with BLEU
An illustrated comparison of 102 languages from X to English or Chinese between BigTrans, ChatGPT and Google Translate. We sort the language scores in BLEU for BigTrans in descending order.

![image](./pics/104langs_bleu.png)

### üåó Automatic Evaluation with GPT-4
An illustrated comparison of 70 languages from X to English or Chinese between BigTrans, ChatGPT and Google Translate. We sort the language scores in GPT-4 score for BigTrans in descending order.

![image](./pics/70langs_gpt4.png)

##  ü§ñ BigTrans Model

### ‚ö†Ô∏è User Notice (Must Read)

<!-- The official [LLaMA models released by Facebook prohibit commercial use](https://github.com/facebookresearch/llama), and the official model weights have not been open-sourced (although there are many third-party download links available online). -->

The BigTrans Model weights are based on [GNU General Public License v3.0](https://www.gnu.org/licenses/gpl-3.0.html) protocols, which is only for research use and cannot be used for commercial purposes. 

***Please confirm that you are using the model in this warehouse with [permission](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform?usp=send_form).***

### üìé Model Download

**BigTrans**Ôºö[Hugging Face](https://huggingface.co/James-WYang/BigTrans) 
<!-- [Google Drive](https://drive.google.com/drive/folders/1r_X7sehOZ1g_an26EziuOrf7G8Q0DjB_?usp=drive_link) -->

<!-- > ‚è≥ Model is uploading -->

### üìå Model Inference
Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

Example usage:

  ```bash
  python -u model/inference.py \
    --model ${CHECKPOINT_PATH} \
    --tokenizer-path ${TOKENIZER_PATH} \
    --prompt-file ${PROMPT_FILE} \
    --with-instruct \
    --out-file ${LOW_OUT_FILE} \
    --seed ${SEED} \
    --beam-search \
    --num-beams ${NUM_BEAMS} \
    --times ${OUT_TIME} \
    --max-tokens ${MAX_TOKENS} \
    --no-repeat-ngram-size ${NO_REPEAT_NGRAM_SIZE} \
    --top-k ${TOP_K} \
    --top-p ${TOP_P} \
    --temperature ${TEMPERATURE} 2>&1 >>${LOG_FILE}
  ```
We can customize the hyperparameters:

  ```bash
  python -u model/inference.py \
    --model ${CHECKPOINT_PATH} \
    --tokenizer-path ${TOKENIZER_PATH} \
    --prompt-file ${PROMPT_FILE} \
    --with-instruct \
    --out-file ${BEAM_OUT_FILE} \
    --seed ${SEED} \
    --beam-search \
    --num-beams 5 \
    --times 1 \
    --max-tokens 256 \
    --no-repeat-ngram-size 6 2>&1 >>${LOG_FILE}
  ```

## License

Our code and documents are released under Apache Licence 2.0

Following LLaMA, our pre-trained weights are released under GNU General Public License v3.0

## Acknowledgement

We thank all contributors for BigTrans projects.

This repo benefits from [LLaMA](https://github.com/facebookresearch/llama), [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca). Thanks for their wonderful works.

## Contact

If you have any questions, please feel free to contact us by sending an email to {yangwen2023, lichong2021}@ia.ac.cn, {jjzhang, cqzong}@nlpr.ia.ac.cn .

## Citation

```
@article{yang-etal-2023-BigTrans,
  author    = {Wen Yang and
               Chong Li and
               Jiajun Zhang and
               Chengqing Zong},
  title={BigTrans: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages},
  journal={arXiv preprint arXiv:2305.18098},
  url={https://arxiv.org/abs/2305.18098},
  year={2023}
}
```




